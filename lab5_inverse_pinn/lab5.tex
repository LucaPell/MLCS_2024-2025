\documentclass{article}
\usepackage[left = 20mm, right = 20mm]{geometry}
\linespread{1.2}
\usepackage{xcolor,eso-pic,soul}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{mathtools}
\graphicspath{{./figures}}
\definecolor{bluunipv}{HTML}{1E489F}
\newcommand{\step}[1]{\underline{\textbf{\large{#1}}} }  
\setulcolor{bluunipv}
\setul{0.5ex}{0.3ex}
\title{\textcolor{white}{\textbf{Machine Learning per il Calcolo Scientifico}\\ \small\textbf{Fifth laboratory exercises }}}

\date{}

% \AddToShipoutPictureBG{\color{bluunipv}%
% \AtPageUpperLeft{\rule[-20mm]{\paperwidth}{30cm}}%
% }

\begin{document}
\definecolor{rossounipv}{HTML}{B2264A}
\definecolor{bluunipv}{HTML}{1E489F}
%\pagecolor{bluunipv}
\AddToShipoutPicture*
{%
  \AtPageUpperLeft
  {%
    \color{bluunipv}%
    \raisebox{-.1\paperheight}{\rule{\paperwidth}{.5\paperheight}}%

  }%
  %\color{white}%
  % \rule{\paperwidth}{.5\paperheight}%
}
\newgeometry{left = 20mm, right = 20mm, top = -6mm}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}\step{General guide} \end{center}
\noindent
For each laboratory, an incomplete Python notebook will be provided with exercises (steps) that must be completed in the given order (some of the exercises will be needed in future laboratories). In step zero, all the Python packages that are needed in order to complete the notebook are listed. This PDF includes the text for the exercises and the expected outcomes for each step. While following the notebook is recommended, you are also welcome to attempt the exercises without using it.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}\step{Step Zero} \end{center}

Here are the required Python (\url{https://www.python.org/}) packages for this laboratory:
\begin{itemize}
  \item PyTorch (\url{https://pytorch.org/})
  \item Numpy (\url{https://numpy.org/})
  \item Matplotlib (\url{https://matplotlib.org/})
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}\step{Step one: trivial inverse problem}\end{center}
We aim to find the scalar parameter $\alpha \in \mathbb{R}$ given a target function $\tilde{u}$. Specifically, we want to solve the following optimization problem:
$$
  \text{arg}\min f(u,\tilde{u}) = \| u - \tilde{u} \|^2,
$$
\noindent
where $u$ satisfies the boundary value problem described by a second-order partial differential equation (PDE):
\[
  \begin{cases}
    -\alpha \frac{d^2u}{dx^2} = f(x), \quad x \in \Omega = (0,1), \\
    u(0) = u(1) =  0.
  \end{cases}
\]
\noindent
The target function $\tilde{u}$ is given by:
$$\tilde{u} = \sin(\pi x)$$
\noindent
The function $f(x)$, derived from the assumed true solution, is specified as:
$$f = 2 \pi^2 \sin(\pi x).$$
\noindent
Our goal is to compute the value of $\alpha$ that best aligns the solution $u(x)$ of the PDE with the given $\tilde{u}(x)$.
To achieve this, we can utilize a Physics-Informed Neural Network (PINN) that integrates the differential equation as a part of its loss function.
This approach ensures that the learned solution not fits the data while respecting the underlying "physics" described by the PDE.
\begin{itemize}
  \item[a.] Define the PINN model with 1 neuron in input, 1 neuron in output and 4 hidden layers with 50 neurons each. Use the tanh activation function. Then define a parameter $\alpha$ as a tensor with requires\_grad=True that will be trained in order to approximate the true value of $\alpha$.
  \item[b.] Define the boundary conditions take 150 random points for the train of the PINN, 60 equispaced points for the data interpolation and 100 points for the test.
  \item[c.] Define the exact solution, the right-hand side of the PDE and the loss function for the PINN and the exact value of $\alpha$.
  \item[d.] Define the derivative of the Neural Network. Then try to train the network and plot the results obtained.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}\step{Step two: Define the FNO model}\end{center}
\begin{itemize}
  \item[a.] Define the spectral convolution block, this correspond to do the Fourier transform of the input, multiply with a coefficients tensor and then do the inverse Fourier transform of the result. For the Fourier transform and inverse Fourier transform you can use the functions 'torch.fft.rfft' and 'torch.fft.irfft' respectively.

  \item[b.] Define the Fourier Neural Operator, this correspond to apply the spectral convolution block multiple times with the lifitng operator at the beginning and the projection operator at the end.

  \item[c.] Introduce a FNO with 3 layers, tanh activation function, hidden dimension (width) equal to 64 and 16 Fourier modes.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\restoregeometry
\begin{center}\step{Step three: Train the FNO}\end{center}
\begin{itemize}
  \item[a.] Define a function to train the FNO using the Adam optimizer with a learning rate of $0.001$ for 50 epochs and halving the learning rate every 50 epochs and use the relative $L^2$ norm as loss function (for the loss use the function LprelLoss provided in the utilities.py file). During the training process save the value of the loss function.
  \item[b.] Make a figure of the loss function and check that it decreases.
  \item[c.] Plot an example of the approximation with the corresponding true solution and check the results obtained.
  \item[d.] You can run the training process multiple times (without restarting the notebook) to augment the number of epochs of training and check both about the convergence of the loss function and the resulting approximation plots.
\end{itemize}

\end{document}
